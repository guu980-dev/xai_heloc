{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import inspect\n",
    "import lime\n",
    "import lime.lime_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/heloc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징과 라벨 분리 (헬로크 데이터에서 y값을 'RiskPerformance'로 가정)\n",
    "X = df.drop(columns=['RiskPerformance'])  # 입력 변수\n",
    "y = df['RiskPerformance'].apply(lambda x: 1 if x == 'Bad' else 0)  # 'Bad'를 1로, 'Good'을 0으로 변환\n",
    "\n",
    "# 학습/테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost용 데이터 형식 변환\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test, y_test)\n",
    "\n",
    "# # 기본 하이퍼파라미터 설정\n",
    "# param = {'silent':True, 'objective':'binary:logistic', \"eta\":0.05, 'eval_metric': 'rmse',\n",
    "#          'monotone_constraints':\"(1,1,1,1,-1,-1,1,0,0,-1,-1,-1,0,-1,0,1,1)\"}\n",
    "\n",
    "# # Cross-validation으로 적절한 boosting round 찾기\n",
    "# bst_cv = xgb.cv(param, dtrain, 500, nfold=10, early_stopping_rounds=10)\n",
    "\n",
    "# # 모델 훈련\n",
    "# evals_result = {}\n",
    "# evallist  = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "# bst = xgb.train(param, dtrain, num_boost_round=bst_cv.shape[0], evals_result=evals_result, evals=evallist, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster()\n",
    "bst.load_model('model/xgb.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "y_train_pred = bst.predict(dtrain)\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "# predictions1 = [round(value) for value in y_train_pred]\n",
    "# predictions2 = [round(value) for value in y_test_pred]\n",
    "\n",
    "# print('Train accuracy:', accuracy_score(y_train, predictions1))\n",
    "# print('Test accuracy:', accuracy_score(y_test, predictions2))\n",
    "\n",
    "# AUC Score 계산\n",
    "# auc_train = roc_auc_score(y_train, y_train_pred)  # 확률 기반으로 AUC 계산\n",
    "# auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "# print('AUC_Train:', auc_train.round(4))\n",
    "# print('AUC_Test:', auc_test.round(4))\n",
    "\n",
    "# print(y_test_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP, LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(x_train.values,\n",
    "                                                        feature_names=list(x_train.columns),\n",
    "                                                        class_names=['Good', 'Bad'],\n",
    "                                                        discretize_continuous=True)\n",
    "explainer = shap.TreeExplainer(bst)\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "# XGBoost의 예측을 확률로 반환하도록 하는 래핑된 함수\n",
    "def predict_fn_for_lime(input_data):\n",
    "    dmatrix_data = xgb.DMatrix(input_data, feature_names=list(x_train.columns))\n",
    "    # 확률 값으로 반환하여 LIME에서 사용 가능하게 함\n",
    "    probas = bst.predict(dmatrix_data)  # 확률 값 반환\n",
    "    return np.column_stack([1 - probas, probas])  # LIME은 클래스별 확률을 받으므로 [1-p, p] 형식으로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = inspect.cleandoc('''\n",
    "Question:\n",
    "The following is the result of binary classification using the HELOC (Home Equity Line of Credit) Dataset and XGBClassifier to classify RiskPerformance into “Good” and “Bad.”\n",
    "The value “Bad” indicates that a consumer was 90 days past due or worse at least once over a period of 24 months from when the credit account was opened. The value “Good” indicates that they have made their payments without ever being more than 90 days overdue.\n",
    "\n",
    "Before answering, please think steyp by step coincisely in these steps to explain the prediction.\n",
    "1. SHAP Analysis: Analyze the key features from the SHAP analysis, explaining how each feature contributes to the prediction.\n",
    "This step should be inside <SHAP>$$INSERT TEXT HERE$$</SHAP> tag.\n",
    "2. LIME Analysis: Analyze the key features from the LIME analysis, explaining the contribution of each feature in terms of how it influences the prediction.\n",
    "This step should be inside <LIME>$$INSERT TEXT HERE$$</LIME> tag.\n",
    "3. Insight Synthesis: Based on the individual feature analyses from SHAP and LIME, synthesize the insights to provide a comprehensive conclusion. The conclusion should focus on how these features work together to influence the final prediction.\n",
    "This step should be inside <Insight>$$INSERT TEXT HERE$$</Insight> tag.\n",
    "4. Final Explanation for Non-Experts: Provide the prediction result and explain the comprehensive reasoning behind the result, considering multiple factors that contributed to this outcome. Ensure the explanation is clear, detailed, and avoids using technical terms or direct references to probabilities or numbers, so that the final explanation is understandable to non-experts in machine learning or finance.\n",
    "This step should be inside <Conclusion>$$INSERT TEXT HERE$$</Conclusion> tag.\n",
    "In this part,\n",
    "- Ensure to be thorough and specific as possible, with enough length to fully explain the reasoning behind the prediction and offer clear, actionable advice to the user.\n",
    "- Please respond as if you were a human, using natural conversational tone. Be engaging, empathetic, and use phrases and expressions that sound like they’re coming from a real person, keeping the tone friendly and conversational. Avoid sounding overly formal or robotic.\n",
    "- Please provide a sentences without explicitly using terms like 'model,' 'probability,' or directly mentioning numbers. Instead, explain the concepts in simple, intuitive language that avoids technical jargon.\n",
    "- At the end of the part, provide a personalized piece of advice for the user on how they can improve or maintain their risk performance in the future.\n",
    "\n",
    "Context:\n",
    "1. Prediction Probability\n",
    "- Good: {predict_proba_good}\n",
    "- Bad: {predict_proba_bad}\n",
    "- Predicted to {predicted_class}\n",
    "\n",
    "2. SHAP analysis (Feature, SHAP Importance)\n",
    "{shap_analysis}\n",
    "\n",
    "3. LIME analysis (Feature, LIME Importance)\n",
    "{lime_analysis}\n",
    "\n",
    "Answer:\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_to_str(df, analysis_type):\n",
    "  if analysis_type == \"SHAP\":\n",
    "    value = \"SHAP Importance\"\n",
    "  elif analysis_type == \"LIME\":\n",
    "    value = \"LIME Importance\"\n",
    "  return '\\n'.join([f\"- ({row['Feature']}, {row[value]})\" for _, row in df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_to_class(proba):\n",
    "  return \"Good\" if proba < 0.5 else \"Bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Feature  SHAP Importance\n",
      "17    NetFractionRevolvingBurden         0.358624\n",
      "7         PercentTradesNeverDelq         0.337536\n",
      "14  MSinceMostRecentInqexcl7days         0.288519\n",
      "9       MaxDelq2PublicRecLast12M         0.235673\n",
      "4          NumSatisfactoryTrades         0.167861\n",
      "18      NetFractionInstallBurden         0.124556\n",
      "15                  NumInqLast6M         0.120804\n",
      "19    NumRevolvingTradesWBalance         0.096290\n",
      "22         PercentTradesWBalance         0.079032\n",
      "12        NumTradesOpeninLast12M         0.076880\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                     Feature  LIME Importance\n",
      "0      MSinceMostRecentInqexcl7days <= -7.00        -0.186296\n",
      "1         NetFractionRevolvingBurden > 54.00         0.169774\n",
      "2             NumSatisfactoryTrades <= 12.00         0.150673\n",
      "3    6.00 < MaxDelq2PublicRecLast12M <= 7.00        -0.135522\n",
      "4   96.00 < PercentTradesNeverDelq <= 100.00        -0.116345\n",
      "5                    NumTotalTrades <= 12.00         0.048523\n",
      "6        NumTrades90Ever2DerogPubRec <= 0.00        -0.031774\n",
      "7      0.00 < NumTradesOpeninLast12M <= 1.00        -0.018633\n",
      "8  3.00 < NumRevolvingTradesWBalance <= 5.00        -0.015197\n",
      "9     49.50 < PercentTradesWBalance <= 67.00        -0.012474\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0.7858818\n",
      "- (MSinceMostRecentInqexcl7days <= -7.00, -0.1862958262204174)\n",
      "- (NetFractionRevolvingBurden > 54.00, 0.1697744086660567)\n",
      "- (NumSatisfactoryTrades <= 12.00, 0.15067251714408816)\n",
      "- (6.00 < MaxDelq2PublicRecLast12M <= 7.00, -0.13552202595656412)\n",
      "- (96.00 < PercentTradesNeverDelq <= 100.00, -0.11634458668076612)\n",
      "- (NumTotalTrades <= 12.00, 0.048522799424500274)\n",
      "- (NumTrades90Ever2DerogPubRec <= 0.00, -0.03177424916023367)\n",
      "- (0.00 < NumTradesOpeninLast12M <= 1.00, -0.01863270943047522)\n",
      "- (3.00 < NumRevolvingTradesWBalance <= 5.00, -0.015196584463750018)\n",
      "- (49.50 < PercentTradesWBalance <= 67.00, -0.012473707501541172)\n",
      "- (NetFractionRevolvingBurden, 0.3586236834526062)\n",
      "- (PercentTradesNeverDelq, 0.3375359773635864)\n",
      "- (MSinceMostRecentInqexcl7days, 0.2885185480117798)\n",
      "- (MaxDelq2PublicRecLast12M, 0.2356729656457901)\n",
      "- (NumSatisfactoryTrades, 0.16786108911037445)\n",
      "- (NetFractionInstallBurden, 0.12455616891384125)\n",
      "- (NumInqLast6M, 0.12080446630716324)\n",
      "- (NumRevolvingTradesWBalance, 0.09629019349813461)\n",
      "- (PercentTradesWBalance, 0.0790317952632904)\n",
      "- (NumTradesOpeninLast12M, 0.07687996327877045)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "  lime_exp = lime_explainer.explain_instance(x_test.iloc[i].values, predict_fn_for_lime, num_features=10)\n",
    "  lime_importance = pd.DataFrame(lime_exp.as_list(), columns=['Feature', 'LIME Importance'])\n",
    "  lime_importance_string = importance_to_str(lime_importance, \"LIME\")\n",
    "  \n",
    "  shap_importance = pd.DataFrame({'Feature': x_test.columns, 'SHAP Importance': np.abs(shap_values[1])})\n",
    "  shap_importance = shap_importance.sort_values(by='SHAP Importance', ascending=False).head(10)\n",
    "  shap_importance_string = importance_to_str(shap_importance, \"SHAP\")\n",
    "  # Should convert to plain text\n",
    "\n",
    "  prediction = y_test_pred[i]\n",
    "  prediction_class = proba_to_class(y_test_pred[i])\n",
    "\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import inspect\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(inputs[\"input_ids\"], max_length=3000, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heloc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
